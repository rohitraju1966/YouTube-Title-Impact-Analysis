{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# A Data Driven YouTube Title Ranking System For Content Creators"
      ],
      "metadata": {
        "id": "FUKKcSSa8SS3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-32Bl9TvGHjx"
      },
      "source": [
        "# Importing essential libraries and data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oONo1cIUEHxq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "data=pd.read_csv(\"/content/drive/MyDrive/Data for stats and DM/Youtube_Data_New.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMdsaYUaGMb-"
      },
      "source": [
        "# Text Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpDIYH3dEaH_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Code to encode text using BERT model\n",
        "def encode_text_with_bert(text, model='bert-base-uncased'):\n",
        "    tokenizer = BertTokenizer.from_pretrained(model)\n",
        "    model_1 = BertModel.from_pretrained(model)\n",
        "    t = tokenizer(text, return_tensors='pt')\n",
        "    val = model_1(**t)\n",
        "    embeddings = val.last_hidden_state.detach().numpy()\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "Embeddings = []\n",
        "\n",
        "for title in tqdm(data['Title'], desc='Processing Titles', unit='title'):\n",
        "    embeddings = encode_text_with_bert(title)\n",
        "    Embeddings.append(embeddings)\n",
        "\n",
        "Embeddings = np.array(Embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlZBCE4_Gxus"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "Embeddings = []\n",
        "for title in tqdm(data['Title'], desc='Processing Titles', unit='title'):\n",
        "    embeddings = encode_text_with_bert(title)\n",
        "    Embeddings.append(embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyzmYk3uGX5l"
      },
      "source": [
        "# Regression model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalization"
      ],
      "metadata": {
        "id": "YmN5Ccfr4FhQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4bi03Dvl-LY"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# Z scaled\n",
        "scaler = StandardScaler()\n",
        "k=np.array(data['Views'])\n",
        "views_new = k.reshape(-1, 1)\n",
        "views_z_score = scaler.fit_transform(views_new).flatten()\n",
        "\n",
        "# Min-Max normalization\n",
        "def min_max_scaling(val):\n",
        "    min_val = np.min(val)\n",
        "    max_val = np.max(val)\n",
        "    scaled_data = (val-min_val)/(max_val-min_val)\n",
        "    return scaled_data\n",
        "\n",
        "views_minmax_score = min_max_scaling(data['Views'])\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59_FYxJ7yCEQ"
      },
      "source": [
        "## Regression Experiment with Z scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJjWPVu2x9z3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#Setting decive to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "data = pd.read_csv(\"Youtube_Data_New.csv\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-large-cased\")\n",
        "titles = list(data['Title'])\n",
        "\n",
        "# Z scaled\n",
        "scaler = StandardScaler()\n",
        "k=np.array(data['Views'])\n",
        "views_new = k.reshape(-1, 1)\n",
        "views_z_score = scaler.fit_transform(views_new).flatten()\n",
        "\n",
        "#text encoder\n",
        "encoding = tokenizer(titles, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "labels = torch.tensor(views_z_score, device=device).view(-1, 1)\n",
        "\n",
        "data = TensorDataset(encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device), labels)\n",
        "\n",
        "train_size = int(0.8 * len(data))\n",
        "val_size = len(data) - train_size\n",
        "train_data, val_data = random_split(data, [train_size, val_size])\n",
        "\n",
        "# DataLoader to form train and validation datasets\n",
        "train_dataloader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "val_dataloader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=1)\n",
        "model = model.to(device)  # Move the model to GPU\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "for epoch in range(10):  # Number of training epochs\n",
        "    print(\"EPOCH: \",epoch)\n",
        "    for batch in train_dataloader:\n",
        "        inputs, masks, labels = batch\n",
        "        inputs, masks, labels = inputs.to(device), masks.to(device), labels.to(device)\n",
        "        masks = masks.to(torch.float32)\n",
        "        labels = labels.to(torch.float32)\n",
        "        outputs = model(inputs, attention_mask=masks, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "model.eval()\n",
        "val_los = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in val_dataloader:\n",
        "        inputs, masks, labels = batch\n",
        "        outputs = model(inputs, attention_mask=masks, labels=labels)\n",
        "        val_los.append(outputs.loss.item())\n",
        "\n",
        "mean_val_loss = sum(val_los) / len(val_los)\n",
        "print(f'Mean Validation Loss: {mean_val_loss}')\n",
        "\n",
        "\n",
        "model.eval()\n",
        "# Example: Predictions for the first batch in the validation set\n",
        "with torch.no_grad():\n",
        "    inputs, masks, _ = next(iter(val_dataloader))\n",
        "    predictions = model(inputs, attention_mask=masks).logits\n",
        "\n",
        "print(\"Predictions:\", predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression Experiment with Min-Max scalled"
      ],
      "metadata": {
        "id": "OYFYtBp155GV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#Setting decive to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "data = pd.read_csv(\"Youtube_Data_New.csv\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-large-cased\")\n",
        "titles = list(data['Title'])\n",
        "\n",
        "# Min-max scaled\n",
        "def min_max_scaling(val):\n",
        "    min_val = np.min(val)\n",
        "    max_val = np.max(val)\n",
        "    scaled_data = (val-min_val)/(max_val-min_val)\n",
        "    return scaled_data\n",
        "\n",
        "views_minmax_score = min_max_scaling(data['Views'])\n",
        "\n",
        "#text encoder\n",
        "encoding = tokenizer(titles, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "labels = torch.tensor(views_minmax_score, device=device).view(-1, 1)\n",
        "\n",
        "data = TensorDataset(encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device), labels)\n",
        "\n",
        "train_size = int(0.8 * len(data))\n",
        "val_size = len(data) - train_size\n",
        "train_data, val_data = random_split(data, [train_size, val_size])\n",
        "\n",
        "# DataLoader to form train and validation datasets\n",
        "train_dataloader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "val_dataloader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=1)\n",
        "model = model.to(device)  # Move the model to GPU\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "for epoch in range(10):  # Number of training epochs\n",
        "    print(\"EPOCH: \",epoch)\n",
        "    for batch in train_dataloader:\n",
        "        inputs, masks, labels = batch\n",
        "        inputs, masks, labels = inputs.to(device), masks.to(device), labels.to(device)\n",
        "        masks = masks.to(torch.float32)\n",
        "        labels = labels.to(torch.float32)\n",
        "        outputs = model(inputs, attention_mask=masks, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "model.eval()\n",
        "val_los = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in val_dataloader:\n",
        "        inputs, masks, labels = batch\n",
        "        outputs = model(inputs, attention_mask=masks, labels=labels)\n",
        "        val_los.append(outputs.loss.item())\n",
        "\n",
        "mean_val_loss = sum(val_los) / len(val_los)\n",
        "print(f'Mean Validation Loss: {mean_val_loss}')\n",
        "\n",
        "\n",
        "model.eval()\n",
        "# Example: Predictions for the first batch in the validation set\n",
        "with torch.no_grad():\n",
        "    inputs, masks, _ = next(iter(val_dataloader))\n",
        "    predictions = model(inputs, attention_mask=masks).logits\n",
        "\n",
        "print(\"Predictions:\", predictions)"
      ],
      "metadata": {
        "id": "gdwgAzg55rhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFVL9t9rXrth"
      },
      "source": [
        "# BERT for classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4MdWc9EYmh3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#Pre-processing data for classification\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/Data for stats and DM/Youtube_Data_New.csv\")\n",
        "bins = [0, 1000, 10000, 100000, 500000]  # Adjusted bins\n",
        "labels = ['Week title (0-1000 views)', 'Could be better (1000-10000 views)', 'Good title (10000-100000 views)', 'Very strong title (100000-500000 views)']\n",
        "data['views_category'] = pd.cut(data['Views'], bins=bins, labels=labels, include_lowest=True)\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-large-cased\")\n",
        "\n",
        "# Inputs\n",
        "texts = list(data['Title'])\n",
        "encoding = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# Outputs\n",
        "views_categories = data['views_category']\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_data = label_encoder.fit_transform(views_categories)\n",
        "labels = torch.tensor(encoded_data, device=device).view(-1)\n",
        "\n",
        "data = TensorDataset(encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device), labels)\n",
        "\n",
        "train_size = int(0.9*len(data))\n",
        "val_size = len(data)-train_size\n",
        "train_dataset, val_dataset = random_split(data, [train_size, val_size])\n",
        "\n",
        "# DataLoader without pin_memory when working with GPU tensors\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/Data_mining_model\", num_labels=len(labels.unique()))\n",
        "model = model.to(device)  # Move the model to GPU\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "cross_entropy = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Lists to store training and validation scores for each epoch\n",
        "train_accuracies = []\n",
        "train_precisions = []\n",
        "train_recalls = []\n",
        "train_f1_scores = []\n",
        "val_accuracies = []\n",
        "val_precisions = []\n",
        "val_recalls = []\n",
        "val_f1_scores = []\n",
        "\n",
        "for epoch in range(5):  # Number of training epochs\n",
        "    print(\"EPOCH: \", epoch)\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "\n",
        "    true_train = []\n",
        "    predicted_train = []\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        inputs, masks, targets = batch\n",
        "        inputs, masks, targets = inputs.to(device), masks.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs, attention_mask=masks)\n",
        "        logits = outputs.logits\n",
        "        loss = cross_entropy(logits, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        true_train.extend(targets.cpu().numpy())\n",
        "\n",
        "        # Computing predicted labels for the current batch\n",
        "        probabilities_train = torch.nn.functional.softmax(logits, dim=1)\n",
        "        _, predicted_classes_train = torch.max(probabilities_train, dim=1)\n",
        "        predicted_train.extend(predicted_classes_train.cpu().numpy())\n",
        "\n",
        "    train_accuracy = accuracy_score(true_train, predicted_train)\n",
        "    train_precision, train_recall, train_f1, _ = precision_recall_fscore_support(true_train, predicted_train, average='weighted')\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    train_precisions.append(train_precision)\n",
        "    train_recalls.append(train_recall)\n",
        "    train_f1_scores.append(train_f1)\n",
        "\n",
        "    model.eval()\n",
        "    val_losses = []\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            inputs, masks, labels = batch\n",
        "            outputs = model(inputs, attention_mask=masks)\n",
        "            logits = outputs.logits\n",
        "            probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
        "            _, predicted_classes = torch.max(probabilities, dim=1)\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "            predicted_labels.extend(predicted_classes.cpu().numpy())\n",
        "\n",
        "    val_accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "    val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='weighted')\n",
        "    val_accuracies.append(val_accuracy)\n",
        "    val_precisions.append(val_precision)\n",
        "    val_recalls.append(val_recall)\n",
        "    val_f1_scores.append(val_f1)\n",
        "\n",
        "    print(f'Training Loss for epoch {epoch + 1} is: {sum(train_losses) / len(train_losses):.4f}')\n",
        "    print(f'Training Accuracy for epoch {epoch + 1} is: {train_accuracy:.4f}')\n",
        "    print(f'Training Precision for epoch {epoch + 1} is: {train_precision:.4f}')\n",
        "    print(f'Training Recall for epoch {epoch + 1} is: {train_recall:.4f}')\n",
        "    print(f'Training F1 Score for epoch {epoch + 1} is: {train_f1:.4f}')\n",
        "\n",
        "    print(f'Validation Accuracy for epoch {epoch + 1} is: {val_accuracy:.4f}')\n",
        "    print(f'Validation Precision for epoch {epoch + 1} is: {val_precision:.4f}')\n",
        "    print(f'Validation Recall for epoch {epoch + 1} is: {val_recall:.4f}')\n",
        "    print(f'Validation F1 Score for epoch {epoch + 1} is: {val_f1:.4f}')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}